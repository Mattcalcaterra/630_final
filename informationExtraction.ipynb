{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# load utalities\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# load dataset tools\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# preprocessing tools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# load models\n",
    "from TorchCRF import CRF\n",
    "\n",
    "\n",
    "# load eval tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# load tokenizing tools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract handout.txt from each subdirectory of RawData\n",
    "def read_handout_txt():\n",
    "    data = []\n",
    "\n",
    "    for root, dirs, files in os.walk(\"./data/RawData/\"):\n",
    "        try:\n",
    "            with open(os.path.join(root, \"handout.txt\"), \"r\") as f:\n",
    "                handout = f.readlines()\n",
    "        except:\n",
    "            print(f\"{root}/handout.txt Not Found\")\n",
    "            continue\n",
    "\n",
    "        for i, line in enumerate(handout):\n",
    "            line = line.strip()\n",
    "\n",
    "            # number lines\n",
    "            line_dict = {\n",
    "                \"Drug name\": root.split(\"/\")[-1],\n",
    "                \"Line number\": i + 1,\n",
    "                \"Line\": line,\n",
    "            }\n",
    "\n",
    "            data.append(line_dict)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_df = pd.read_csv(\"data/AnnotatedData/AnnotatedDUGData.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/RawData//handout.txt Not Found\n",
      "./data/RawData/Coreg/handout.txt Not Found\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.DataFrame(read_handout_txt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug name</th>\n",
       "      <th>Line number</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>1</td>\n",
       "      <td>Patient Educationaripiprazole intramuscular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>2</td>\n",
       "      <td>IMPORTANT: HOW TO USE THIS INFORMATION:  This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>3</td>\n",
       "      <td>ARIPIPRAZOLE EXTENDED RELEASE - INJECTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>4</td>\n",
       "      <td>(AR-i-PIP-ra-zole)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>5</td>\n",
       "      <td>COMMON BRAND NAME(S): Abilify Maintena, Aristada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Drug name  Line number                                               Line\n",
       "0   Abilify            1        Patient Educationaripiprazole intramuscular\n",
       "1   Abilify            2  IMPORTANT: HOW TO USE THIS INFORMATION:  This ...\n",
       "2   Abilify            3          ARIPIPRAZOLE EXTENDED RELEASE - INJECTION\n",
       "3   Abilify            4                                 (AR-i-PIP-ra-zole)\n",
       "4   Abilify            5   COMMON BRAND NAME(S): Abilify Maintena, Aristada"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug name</th>\n",
       "      <th>Drug number</th>\n",
       "      <th>Advice Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>Avoid alcoholic beverages.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Drug name  Drug number                                        Advice Text\n",
       "0   Abilify            0  To reduce the risk of dizziness and lightheade...\n",
       "1   Abilify            0  This medication may rarely make your blood sug...\n",
       "2   Abilify            0  This medication may rarely cause a condition k...\n",
       "3   Abilify            0  This drug may make you dizzy or drowsy or caus...\n",
       "4   Abilify            0                         Avoid alcoholic beverages."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_df = anno_df[[\"Drug name\", \"Drug number\", \"Advice Text\"]]\n",
    "anno_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reassign line numbers\n",
    "\n",
    "The line number present in the data is determined based off of scentence structure and not line number. We will locate the Advice text in the raw text, and assign it a new line number label based on the corresponing line. \n",
    "\n",
    "This will help us to assign IOB tags to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_line_number(advice, raw_data_df):\n",
    "\n",
    "    for i, line in raw_data_df.iterrows():\n",
    "        if advice in line[\"Line\"]:\n",
    "            return line[\"Line number\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug name</th>\n",
       "      <th>Drug number</th>\n",
       "      <th>Advice Text</th>\n",
       "      <th>Line number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>Avoid alcoholic beverages.</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Drug name  Drug number                                        Advice Text  \\\n",
       "0   Abilify            0  To reduce the risk of dizziness and lightheade...   \n",
       "1   Abilify            0  This medication may rarely make your blood sug...   \n",
       "2   Abilify            0  This medication may rarely cause a condition k...   \n",
       "3   Abilify            0  This drug may make you dizzy or drowsy or caus...   \n",
       "4   Abilify            0                         Avoid alcoholic beverages.   \n",
       "\n",
       "   Line number  \n",
       "0         17.0  \n",
       "1         20.0  \n",
       "2         21.0  \n",
       "3         31.0  \n",
       "4         31.0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find line number for each advice text\n",
    "anno_df[\"Line number\"] = anno_df[\"Advice Text\"].apply(\n",
    "    lambda x: find_line_number(x, raw_df)\n",
    ")\n",
    "anno_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug name</th>\n",
       "      <th>Drug number</th>\n",
       "      <th>Advice Text</th>\n",
       "      <th>Line number</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abilify</td>\n",
       "      <td>0</td>\n",
       "      <td>Avoid alcoholic beverages.</td>\n",
       "      <td>31.0</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Drug name  Drug number                                        Advice Text  \\\n",
       "0   Abilify            0  To reduce the risk of dizziness and lightheade...   \n",
       "1   Abilify            0  This medication may rarely make your blood sug...   \n",
       "2   Abilify            0  This medication may rarely cause a condition k...   \n",
       "3   Abilify            0  This drug may make you dizzy or drowsy or caus...   \n",
       "4   Abilify            0                         Avoid alcoholic beverages.   \n",
       "\n",
       "   Line number                                               Line  \n",
       "0         17.0  To reduce the risk of dizziness and lightheade...  \n",
       "1         20.0  This medication may rarely make your blood sug...  \n",
       "2         21.0  This medication may rarely cause a condition k...  \n",
       "3         31.0  This drug may make you dizzy or drowsy or caus...  \n",
       "4         31.0  This drug may make you dizzy or drowsy or caus...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge dataframes\n",
    "merged_df = pd.merge(anno_df, raw_df, on=[\"Drug name\", \"Line number\"])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOB tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bioe_tags(text, advice):\n",
    "    # basic tokenization\n",
    "    text_words = text.split()\n",
    "    advice_words = advice.split()\n",
    "\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "\n",
    "    advice_len = len(advice_words)\n",
    "\n",
    "    for i in range(len(text_words)):\n",
    "        if text_words[i : i + advice_len] == advice_words:\n",
    "            # print('found')\n",
    "            start_idx = i\n",
    "            end_idx = i + advice_len\n",
    "            break\n",
    "    # print(start_idx, end_idx)\n",
    "\n",
    "    # create tags\n",
    "    tags = [\"O\"] * len(text_words)\n",
    "    tags[start_idx] = \"B\"\n",
    "    tags[end_idx - 1] = \"E\"\n",
    "    for i in range(start_idx + 1, end_idx - 1):\n",
    "        tags[i] = \"I\"\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags):\n",
    "    mapping = {\"O\": 0, \"B\": 1, \"I\": 2, \"E\": 3}\n",
    "    return [mapping[tag] for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advice</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "      <td>[B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "      <td>[B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "      <td>[B, I, I, I, I, I, I, I, I, I, E, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>[B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avoid alcoholic beverages.</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              advice  \\\n",
       "0  To reduce the risk of dizziness and lightheade...   \n",
       "1  This medication may rarely make your blood sug...   \n",
       "2  This medication may rarely cause a condition k...   \n",
       "3  This drug may make you dizzy or drowsy or caus...   \n",
       "4                         Avoid alcoholic beverages.   \n",
       "\n",
       "                                                text  \\\n",
       "0  To reduce the risk of dizziness and lightheade...   \n",
       "1  This medication may rarely make your blood sug...   \n",
       "2  This medication may rarely cause a condition k...   \n",
       "3  This drug may make you dizzy or drowsy or caus...   \n",
       "4  This drug may make you dizzy or drowsy or caus...   \n",
       "\n",
       "                                              labels  \n",
       "0  [B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...  \n",
       "1  [B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...  \n",
       "2  [B, I, I, I, I, I, I, I, I, I, E, O, O, O, O, ...  \n",
       "3  [B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_df = merged_df.copy()\n",
    "tagged_df[\"labels\"] = tagged_df.apply(\n",
    "    lambda x: generate_bioe_tags(x[\"Line\"], x[\"Advice Text\"]), axis=1\n",
    ")\n",
    "\n",
    "# reframe\n",
    "tagged_df = tagged_df[[\"Advice Text\", \"Line\", \"labels\"]]\n",
    "\n",
    "# rename\n",
    "tagged_df.columns = [\"advice\", \"text\", \"labels\"]\n",
    "\n",
    "tagged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert tagged data to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advice</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "      <td>To reduce the risk of dizziness and lightheade...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "      <td>This medication may rarely make your blood sug...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "      <td>This medication may rarely cause a condition k...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avoid alcoholic beverages.</td>\n",
       "      <td>This drug may make you dizzy or drowsy or caus...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              advice  \\\n",
       "0  To reduce the risk of dizziness and lightheade...   \n",
       "1  This medication may rarely make your blood sug...   \n",
       "2  This medication may rarely cause a condition k...   \n",
       "3  This drug may make you dizzy or drowsy or caus...   \n",
       "4                         Avoid alcoholic beverages.   \n",
       "\n",
       "                                                text  \\\n",
       "0  To reduce the risk of dizziness and lightheade...   \n",
       "1  This medication may rarely make your blood sug...   \n",
       "2  This medication may rarely cause a condition k...   \n",
       "3  This drug may make you dizzy or drowsy or caus...   \n",
       "4  This drug may make you dizzy or drowsy or caus...   \n",
       "\n",
       "                                              labels  \n",
       "0  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "1  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "2  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, ...  \n",
       "3  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode tags\n",
    "tagged_df[\"labels\"] = tagged_df[\"labels\"].apply(lambda x: encode_tags(x))\n",
    "tagged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_random_IOB(text):\n",
    "\n",
    "    # get the length of the text\n",
    "    length = len(text)\n",
    "\n",
    "    # make array of zeros\n",
    "    preds = np.zeros(length)\n",
    "\n",
    "    # get a random number between 0 and the length of the text\n",
    "    random_start = np.random.randint(0, length - 1)\n",
    "    random_stop = np.random.randint(random_start + 1, length)\n",
    "\n",
    "    # set the random start to 1\n",
    "    preds[random_start] = 1\n",
    "\n",
    "    # set the random stop to 3\n",
    "    preds[random_stop] = 3\n",
    "\n",
    "    # set the values in between to 2\n",
    "    preds[random_start + 1 : random_stop] = 2\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, ...\n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, ...\n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "Name: labels, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = tagged_df[\"labels\"].apply(pred_random_IOB)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute the baseline at token level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find the accuracy for evaluating the baseline at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5056711012750547"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(ground_truth, preds):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for true, pred in zip(ground_truth, preds):\n",
    "        for t, p in zip(true, pred):\n",
    "            if t == p:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# calculate the accuracy of the token level predictions\n",
    "calculate_accuracy(tagged_df[\"labels\"], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the baseline at span-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-level precision: 0.28494639680839756\n",
      "Span-level recall: 0.2832972496230585\n",
      "Span-level F1-score: 0.2784944941860884\n"
     ]
    }
   ],
   "source": [
    "# evaluate f1 at the span level\n",
    "def calculate_f1_span_level(ground_truth, preds):\n",
    "    # flatten the list\n",
    "    ground_truth = [tag for tags in ground_truth for tag in tags]\n",
    "    preds = [tag for tags in preds for tag in tags]\n",
    "\n",
    "    # calculate precision, recall, f1\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        ground_truth, preds, average=\"macro\"\n",
    "    )\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "# calculate f1 at the span level\n",
    "precision, recall, f1_score = calculate_f1_span_level(\n",
    "    tagged_df[\"labels\"], preds\n",
    ")\n",
    "\n",
    "print(\"Span-level precision:\", precision)\n",
    "\n",
    "print(\"Span-level recall:\", recall)\n",
    "\n",
    "print(\"Span-level F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tagged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(example, max_length=512):\n",
    "    # Because of the way that the BIOE tagging was setup, we need to make sure that the\n",
    "    # tokenization is aligned with the tagging. This means that we need to tokenize the\n",
    "    # text and then assign the correct label to each token.\n",
    "\n",
    "    text = example[\"text\"].split()\n",
    "    tags = example[\"labels\"]\n",
    "\n",
    "    token_ids = [tokenizer.cls_token_id]\n",
    "    label_alignment = [0]\n",
    "    attention_mask = [1]\n",
    "    special_label_for_subwords = -100\n",
    "\n",
    "    for word, label in zip(text, tags):\n",
    "        subword_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        if len(subword_tokens) > 0:\n",
    "            token_ids.extend(tokenizer.convert_tokens_to_ids(subword_tokens))\n",
    "            # Assign the correct label to the first subword token\n",
    "            label_alignment.append(label)\n",
    "            # Use a special label for subsequent subword tokens\n",
    "            label_alignment.extend(\n",
    "                [special_label_for_subwords] * (len(subword_tokens) - 1)\n",
    "            )\n",
    "            attention_mask.extend([1] * len(subword_tokens))\n",
    "\n",
    "    # add [SEP] token\n",
    "    token_ids.append(tokenizer.sep_token_id)\n",
    "    label_alignment.append(0)\n",
    "    attention_mask.append(1)\n",
    "\n",
    "    # pad to max length\n",
    "    padding_length = max_length - len(token_ids)\n",
    "    token_ids.extend([tokenizer.pad_token_id] * padding_length)\n",
    "    label_alignment.extend([0] * padding_length)\n",
    "    attention_mask.extend([0] * padding_length)\n",
    "\n",
    "    # Make sure everything has correct length\n",
    "    assert len(token_ids) == max_length\n",
    "    assert len(label_alignment) == max_length\n",
    "    assert len(attention_mask) == max_length\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": token_ids,\n",
    "        \"labels\": label_alignment,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(train)\n",
    "val_dataset = datasets.Dataset.from_pandas(val)\n",
    "test_dataset = datasets.Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d454d3182564374bb5ac4e1849cbcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ba24180c2e4d10afab5d8a9d0bf8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93888c8aec24e6fac2225d769168e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "val_dataset = val_dataset.map(preprocess_text)\n",
    "test_dataset = test_dataset.map(preprocess_text)\n",
    "\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"]\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"]\n",
    ")\n",
    "test_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([   0,    1,    2,    2,    2,    2,    2,    2, -100,    2,    2, -100,\n",
       "            2, -100, -100,    2, -100,    2,    2,    2,    3, -100, -100, -100,\n",
       "         -100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'input_ids': tensor([    0,   243,   354, 42230,  1594,  9226, 23310, 10212,   293, 12473,\n",
       "          7805,  1988, 22238,   330,     4, 24514,  6070, 16625, 44914, 23033,\n",
       "          7805,  1988,    12, 35151,     4,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first example input_ids and labels\n",
    "train_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./models/information_extraction/{model_id}\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoModelForTokenClassificationCRF(AutoModelForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, labels=None, token_type_ids=None\n",
    "    ):\n",
    "        outputs = super().forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        if labels is not None:\n",
    "            # During training, we use the CRF layer to calculate the loss\n",
    "            log_likelihood = self.crf(\n",
    "                sequence_output,\n",
    "                labels,\n",
    "                mask=attention_mask.byte(),\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "            loss = -log_likelihood  # Negative log-likelihood\n",
    "            return loss\n",
    "        else:\n",
    "            # During prediction, we decode the best label sequence\n",
    "            prediction = self.crf.decode(\n",
    "                sequence_output, mask=attention_mask.byte()\n",
    "            )\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id, num_labels=4)\n",
    "model = AutoModelForTokenClassificationCRF.from_pretrained(\n",
    "    model_id, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # custom weights to handle class imbalance\n",
    "        class_weights = torch.tensor([1.0, 4.0, 1.0, 4.0]).to(logits.device)\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            weight=class_weights, ignore_index=-100\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, self.model.config.num_labels), labels.view(-1)\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76591abaee7b4b2d82388f794ef522de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3354, 'grad_norm': 11.514975547790527, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.08}\n",
      "{'loss': 1.2369, 'grad_norm': 12.123732566833496, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.16}\n",
      "{'loss': 1.0215, 'grad_norm': 12.068023681640625, 'learning_rate': 3e-06, 'epoch': 0.24}\n",
      "{'loss': 0.5827, 'grad_norm': 8.419600486755371, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 0.2459, 'grad_norm': 1.9941552877426147, 'learning_rate': 5e-06, 'epoch': 0.39}\n",
      "{'loss': 0.1638, 'grad_norm': 0.9868439435958862, 'learning_rate': 6e-06, 'epoch': 0.47}\n",
      "{'loss': 0.1592, 'grad_norm': 1.186097502708435, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.55}\n",
      "{'loss': 0.154, 'grad_norm': 0.9635775685310364, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.63}\n",
      "{'loss': 0.1364, 'grad_norm': 1.0151338577270508, 'learning_rate': 9e-06, 'epoch': 0.71}\n",
      "{'loss': 0.1266, 'grad_norm': 2.5791282653808594, 'learning_rate': 1e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93c1f9abd564148949837f3f7defba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12522900104522705, 'eval_runtime': 4.371, 'eval_samples_per_second': 50.103, 'eval_steps_per_second': 12.583, 'epoch': 0.79}\n",
      "{'loss': 0.1273, 'grad_norm': 0.737683892250061, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.87}\n",
      "{'loss': 0.112, 'grad_norm': 2.1842164993286133, 'learning_rate': 1.2e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0967, 'grad_norm': 1.2556512355804443, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0839, 'grad_norm': 3.1808149814605713, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0717, 'grad_norm': 1.1634185314178467, 'learning_rate': 1.5e-05, 'epoch': 1.18}\n",
      "{'loss': 0.087, 'grad_norm': 0.748416006565094, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0873, 'grad_norm': 1.6133924722671509, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.34}\n",
      "{'loss': 0.1094, 'grad_norm': 0.8199810981750488, 'learning_rate': 1.8e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0698, 'grad_norm': 1.5296849012374878, 'learning_rate': 1.9e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0889, 'grad_norm': 1.1135082244873047, 'learning_rate': 2e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbca8c76a8e4ac1bb376a3d40d28e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07550052553415298, 'eval_runtime': 4.407, 'eval_samples_per_second': 49.694, 'eval_steps_per_second': 12.48, 'epoch': 1.57}\n",
      "{'loss': 0.0727, 'grad_norm': 3.144752264022827, 'learning_rate': 2.1e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0729, 'grad_norm': 0.6598449349403381, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0772, 'grad_norm': 4.083242893218994, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0824, 'grad_norm': 1.417886734008789, 'learning_rate': 2.4e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0735, 'grad_norm': 0.7137104272842407, 'learning_rate': 2.5e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0706, 'grad_norm': 1.1941686868667603, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0775, 'grad_norm': 0.7145252823829651, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0697, 'grad_norm': 1.034682035446167, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0752, 'grad_norm': 1.5058140754699707, 'learning_rate': 2.9e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0605, 'grad_norm': 0.9267035126686096, 'learning_rate': 3e-05, 'epoch': 2.36}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456f7558fc2440d3b0f93196faf1c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08371739089488983, 'eval_runtime': 4.422, 'eval_samples_per_second': 49.525, 'eval_steps_per_second': 12.438, 'epoch': 2.36}\n",
      "{'loss': 0.0651, 'grad_norm': 1.453368902206421, 'learning_rate': 3.1e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0729, 'grad_norm': 0.630334734916687, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0706, 'grad_norm': 0.4381190538406372, 'learning_rate': 3.3e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0668, 'grad_norm': 1.3177576065063477, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0845, 'grad_norm': 1.1741180419921875, 'learning_rate': 3.5e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0717, 'grad_norm': 1.5329148769378662, 'learning_rate': 3.6e-05, 'epoch': 2.83}\n",
      "{'loss': 0.0717, 'grad_norm': 1.4395291805267334, 'learning_rate': 3.7e-05, 'epoch': 2.91}\n",
      "{'loss': 0.0762, 'grad_norm': 0.783348798751831, 'learning_rate': 3.8e-05, 'epoch': 2.99}\n",
      "{'train_runtime': 122.9829, 'train_samples_per_second': 12.392, 'train_steps_per_second': 3.098, 'train_loss': 0.19466114875290963, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=381, training_loss=0.19466114875290963, metrics={'train_runtime': 122.9829, 'train_samples_per_second': 12.392, 'train_steps_per_second': 3.098, 'train_loss': 0.19466114875290963, 'epoch': 3.0})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b742f882584648d7bf9eb547bebafe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction with the trainer on the test text\n",
    "preds = trainer.predict(test_dataset)\n",
    "\n",
    "# get the predicted labels\n",
    "pred_labels = np.argmax(preds.predictions, axis=2)\n",
    "pred_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_preds(pred_labels):\n",
    "    # takes an array of preds and postprocesses them\n",
    "    filled_array = np.copy(pred_labels)\n",
    "    num_examples, width = filled_array.shape\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        first_1_index = np.where(filled_array[i] == 1)[0]\n",
    "        first_3_index = np.where(filled_array[i] == 3)[0]\n",
    "\n",
    "        if len(first_1_index) > 0 and len(first_3_index) > 0:\n",
    "            start_index = first_1_index[0]\n",
    "            end_index = first_3_index[0]\n",
    "\n",
    "            filled_array[i, start_index + 1 : end_index] = 2\n",
    "\n",
    "    return filled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy based on non-padding tokens\n",
    "def calculate_accuracy_non_padding(predictions, labels, attention_mask):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pred, label, mask in zip(predictions, labels, attention_mask):\n",
    "        # print(pred, label, mask)\n",
    "        for p, l, m in zip(pred, label, mask):\n",
    "            if m == 1:\n",
    "                if p == l:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = test_dataset[\"attention_mask\"]\n",
    "\n",
    "# convert pred_labels to tensor\n",
    "pred_labels = torch.tensor(pred_labels).to(device)\n",
    "\n",
    "calculate_accuracy_non_padding(\n",
    "    pred_labels, test_dataset[\"labels\"], test_dataset[\"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate span-level f1\n",
    "precision, recall, f1_score = calculate_f1_span_level(\n",
    "    test_dataset[\"labels\"].to(\"cpu\"), pred_labels.to(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"Span-level precision:\", precision)\n",
    "print(\"Span-level recall:\", recall)\n",
    "print(\"Span-level F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = postprocess_preds(pred_labels)\n",
    "pred_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3801836024058246"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = test_dataset[\"attention_mask\"]\n",
    "\n",
    "# convert pred_labels to tensor\n",
    "pred_labels = torch.tensor(pred_labels).to(device)\n",
    "\n",
    "calculate_accuracy_non_padding(\n",
    "    pred_labels, test_dataset[\"labels\"], test_dataset[\"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-level precision: 0.4072017447273148\n",
      "Span-level recall: 0.6767245561594364\n",
      "Span-level F1-score: 0.4813581391130929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate span-level f1\n",
    "precision, recall, f1_score = calculate_f1_span_level(\n",
    "    test_dataset[\"labels\"].to(\"cpu\"), pred_labels.to(\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"Span-level precision:\", precision)\n",
    "print(\"Span-level recall:\", recall)\n",
    "print(\"Span-level F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract example advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
